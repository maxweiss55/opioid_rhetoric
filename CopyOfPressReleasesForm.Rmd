---
title: "PressReleaseClean"
author: "Max Weiss"
date: "2/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(ProPublicaR)
library(Rcrawler) 
library(magrittr)
library(tidyr)
library(tokenizers)
library(htmlTable)
library(XML)
library(readxl)
library(tidytext)
library(htmltidy)
library(jsonlite)
library(lubridate)
library(textmineR)
library(tm)
library(fuzzyjoin)
library(reshape2)

```

```{r url scrape setup}

#The following scrapes one page of search results

#URL of ProPublica search site for "opioid OR opiate"
url <- paste0("https://votesmart.org/public-statements/NA/C/?section=officials&s=date&search=opioid&p=", i, ".XlIOEhNKjUo")

#Get full table of one page + row number
read <- url %>%
  read_html()

page_table <- read %>%
  html_nodes(xpath='//td') %>%
  html_text() %>%
  as_tibble() %>%
  mutate(index = row_number())

date <- page_table %>%
  filter((index %% 4) == 1) %>%
  rename(date = value) %>%
  select(-index)
title <- page_table %>%
  filter((index %% 4) == 2) %>%
  rename(title = value) %>%
  select(-index)
member <- page_table %>%
  filter((index %% 4) == 3) %>%
  rename(member = value) %>%
  select(-index)

path <- read %>%
  html_nodes(xpath="//a") %>%
  html_attr('href') %>%
  as_tibble() %>%
  filter(str_detect(value, '/public-statement/')) %>%
  mutate(path = paste0("https://votesmart.org/", value)) %>%
  select(-value)

layer <- cbind(cbind(cbind(date, title), member), path) %>%
  mutate(text = NA_character_)


for (j in c(1:100)){

layer$text[j] <- paste(html_text(html_nodes(read_html(layer$path[j]), xpath='//p')), collapse = " ")
}

layer

```

```{r scrape loop and clean}

bigboy <- tibble(date = NA_character_, title = NA_character_, member = NA_character_, path = NA_character_, text = NA_character_)

for (i in c(1:136)){
  
  url <- paste0("https://votesmart.org/public-statements/NA/C/?section=officials&s=date&search=opioid&p=", i, "#.XlIOEhNKjUo")
  
  read <- url %>%
  read_html()

page_table <- read %>%
  html_nodes(xpath='//td') %>%
  html_text() %>%
  as_tibble() %>%
  mutate(index = row_number())

date <- page_table %>%
  filter((index %% 4) == 1) %>%
  rename(date = value) %>%
  select(-index)
title <- page_table %>%
  filter((index %% 4) == 2) %>%
  rename(title = value) %>%
  select(-index)
member <- page_table %>%
  filter((index %% 4) == 3) %>%
  rename(member = value) %>%
  select(-index)

path <- read %>%
  html_nodes(xpath="//a") %>%
  html_attr('href') %>%
  as_tibble() %>%
  filter(str_detect(value, '/public-statement/')) %>%
  mutate(path = paste0("https://votesmart.org/", value)) %>%
  select(-value)

layer <- cbind(cbind(cbind(date, title), member), path) %>%
  mutate(text = NA_character_)


for (j in c(1:100)){

layer$text[j] <- paste(html_text(html_nodes(read_html(layer$path[j]), xpath='//p')), collapse = " ")
print(j)
}
  bigboy <- rbind(bigboy, layer)
  write_rds(bigboy, 'bigboy.rds')
  print(i)
}


bigboy1 <- bigboy %>%
  mutate(text = str_remove_all(text, " All content © 1992 - 2020 Vote Smart unless otherwise attributed - Privacy Policy - Legislative demographic data provided by Aristotle International, Inc. Mobile Version \\#\\{text\\} You are about to be redirected to a secure checkout page. Please note: The total order amount will read \\$0.01. This is a card processor fee. Please know that a recurring donation of the amount and frequency that you selected will be processed and initiated tomorrow. You may see a one-time charge of \\$0.01 on your statement. Continue to secure page »")) %>%
  filter(!str_detect(title, "^Letter to") & !str_detect(title, "^Providing for Consideration"))


write_csv(bigboyclean, 'rawtext_big.csv')
write_rds(bigboy1, 'bigboyclean.rds')


read_rds('bigboy.rds') %>%
  filter(row_number() != 1) %>%
  write_rds('fullscrape.rds')

```


```{r texploration}

##This is the full built dataset
bigboyclean <- read_rds('bigboyclean.rds')
bigboyclean %>% distinct() %>% select(-text)
bigboyclean %>% select(-text) %>% tail(200)

#Processing built dataset
details <- bigboyclean %>% 
  mutate(office = ifelse(str_detect(member, "Sen\\."), "Senate", "House")) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(date = mdy(date))
  
#https://voteview.com/data
#Lewis, Jeffrey B., Keith Poole, Howard Rosenthal, Adam Boche, Aaron Rudkin, and Luke Sonnet (2020). Voteview: Congressional Roll-Call Votes Database. https://voteview.com/

members <- read_xls("member_info.xls")


#Date histogram
details %>% 
ggplot(aes(x=date)) + geom_histogram(binwidth=30, colour="white")

```

```{r all press releases scraped}

bigboy <- tibble(date = NA_character_, title = NA_character_, member = NA_character_)

for (i in c(1:8095)){
  
  url <- paste0("https://votesmart.org/public-statements/NA/C/?section=officials&s=date&p=", i, "#.Xlc41xNKh0s")
  
  read <- url %>%
  read_html()

page_table <- read %>%
  html_nodes(xpath='//td') %>%
  html_text() %>%
  as_tibble() %>%
  mutate(index = row_number())

date <- page_table %>%
  filter((index %% 4) == 1) %>%
  rename(date = value) %>%
  select(-index)
title <- page_table %>%
  filter((index %% 4) == 2) %>%
  rename(title = value) %>%
  select(-index)
member <- page_table %>%
  filter((index %% 4) == 3) %>%
  rename(member = value) %>%
  select(-index)

layer <- cbind(cbind(date, title), member)

  bigboy <- rbind(bigboy, layer)
  write_rds(bigboy, 'allstatements.rds')
  print(i)
}

write_rds('allstatements4048.rds')

bigboy <- read_rds('allstatements.rds')

bigboy %>% nrow()



bigboy <- read_rds('allstatements.rds')

bigboy %>% 
  filter(row_number() != 1) %>%
  distinct(date, title, member) %>%
  write_csv("distinct_scrape.csv")

```


```{r member count analysis}

allstatements <- read_rds('allstatements.rds')

allstatements <- allstatements %>%
  filter(!is.na(date)) %>%
  distinct() %>%
  count(member) %>%
  arrange(desc(n)) %>%
  rename(total = n)
#  filter(!str_detect(title, "^Letter to") & !str_detect(title, "^Providing for Consideration") & !str_detect(title, "^Issue Position")) %>%
#  mutate(first = str_extract(title, '\\w*')) %>%
#  count(first) %>%
#  arrange(desc(n))
  
  
opioid <- read_rds('fullscrape.rds')

opioid <- opioid %>%
  filter(!is.na(date)) %>%
  distinct() %>%
  select(-text) %>%
  count(member) %>%
  arrange(desc(n)) %>%
  rename(opioid = n)


memcounts <- left_join(allstatements, opioid) %>%
  mutate(opioid = ifelse(is.na(opioid), 0, opioid)) %>%
  mutate(prop = opioid / total) %>%
  arrange(desc(prop))

#  mutate(text = str_remove_all(text, " All content © 1992 - 2020 Vote Smart unless otherwise attributed - Privacy Policy - Legislative demographic data provided by Aristotle International, Inc. Mobile Version \\#\\{text\\} You are about to be redirected to a secure checkout page. Please note: The total order amount will read \\$0.01. This is a card processor fee. Please know that a recurring donation of the amount and frequency that you selected will be processed and initiated tomorrow. You may see a one-time charge of \\$0.01 on your statement. Continue to secure page »"))
#  filter(!str_detect(title, "^Letter to") & !str_detect(title, "^Providing for Consideration"))


###JOINING MEMBER IDS

leg <- read_csv("legislators-current.csv")

leg2 <- leg %>% 
  mutate(full_name = str_to_lower(full_name)) %>%
  mutate(full_name = str_replace_all(full_name, 'á', 'a')) %>%
  mutate(full_name = str_replace_all(full_name, 'é', 'e')) %>%
  mutate(full_name = str_replace_all(full_name, 'ó', 'o')) %>%
  mutate(full_name = str_replace_all(full_name, 'í', 'i')) %>%
  mutate(full_name = str_replace_all(full_name, 'ú', 'u')) %>%
  mutate(full_name = str_replace_all(full_name, 'ñ', 'n')) %>%
  mutate(full_name = str_replace_all(full_name, ', jr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' jr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ', sr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' sr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ', iii', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' iii', '')) %>%
  select(last_name, first_name, middle_name, suffix, nickname, full_name) %>%
  arrange(last_name)
  
memcounts2 <- memcounts %>% 
  mutate(office = ifelse(str_detect(member, "Sen\\."), "Senate", "House")) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' ')) %>%
  mutate(last = str_extract(member, '\\w+$')) %>%
  arrange(last)

#These two were used to manually join
write_csv(leg2, "leg.csv")
write_csv(memcounts2, "memcounts.csv")

#Manually joined key
key <- read_xlsx('key.xlsx')

memcountskey <- memcounts %>%
  mutate(office = ifelse(str_detect(member, "Sen\\."), "Senate", "House")) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' ')) %>%
  left_join(key, by = "member")


leg <- read_csv("leg.csv")
legkey <- leg %>% 
  mutate(full_name = str_to_lower(full_name)) %>%
  mutate(full_name = str_replace_all(full_name, 'á', 'a')) %>%
  mutate(full_name = str_replace_all(full_name, 'é', 'e')) %>%
  mutate(full_name = str_replace_all(full_name, 'ó', 'o')) %>%
  mutate(full_name = str_replace_all(full_name, 'í', 'i')) %>%
  mutate(full_name = str_replace_all(full_name, 'ú', 'u')) %>%
  mutate(full_name = str_replace_all(full_name, 'ñ', 'n')) %>%
  mutate(full_name = str_replace_all(full_name, ', jr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' jr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ', sr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' sr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ', iii', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' iii', '')) %>%
  left_join(key, by = "full_name")



meminfo <- left_join(memcountskey, legkey)



df <- read_rds('fullscrape.rds') %>%
  filter(!str_detect(date, "2019") | !str_detect(date, "2020"))%>%
  select(-text) %>%
  count(member) %>%
  arrange(desc(n))

hist(df$n, breaks = 200)


df <- read_rds('fullscrape.rds') 

process <- df %>%
  filter(str_detect(date, "2019") | 
         str_detect(date, "Jan. 3, 2020") | 
         str_detect(date, "Jan. 2, 2020") | 
         str_detect(date, "Jan. 1, 2020")) %>%
  filter(!str_detect(date, "Jan. 2, 2019") & 
         !str_detect(date, "Jan. 1, 2019")) %>%
  mutate(year = NA,
         month = NA,
         day = NA) %>%
  mutate(year = ifelse(str_detect(date, "2019"), "2019", year),
         year = ifelse(str_detect(date, "2020"), "2020", year)) %>%
  mutate(month = ifelse(str_detect(date, "Jan"), "1", month),
         month = ifelse(str_detect(date, "Feb"), "2", month),
         month = ifelse(str_detect(date, "Mar"), "3", month),
         month = ifelse(str_detect(date, "Apr"), "4", month),
         month = ifelse(str_detect(date, "May"), "5", month),
         month = ifelse(str_detect(date, "Jun"), "6", month),
         month = ifelse(str_detect(date, "Jul"), "7", month),
         month = ifelse(str_detect(date, "Aug"), "8", month),
         month = ifelse(str_detect(date, "Sep"), "9", month),
         month = ifelse(str_detect(date, "Oct"), "10", month),
         month = ifelse(str_detect(date, "Nov"), "11", month),
         month = ifelse(str_detect(date, "Dec"), "12", month)) %>%
  mutate(day = str_extract(date, "[[:digit:]]+")) %>%
  mutate(text = str_remove_all(text, " All content © 1992 - 2020 Vote Smart unless otherwise attributed - Privacy Policy - Legislative demographic data provided by Aristotle International, Inc. Mobile Version \\#\\{text\\} You are about to be redirected to a secure checkout page. Please note: The total order amount will read \\$0.01. This is a card processor fee. Please know that a recurring donation of the amount and frequency that you selected will be processed and initiated tomorrow. You may see a one-time charge of \\$0.01 on your statement. Continue to secure page »")) %>%
  mutate(office = ifelse(str_detect(member, "Sen\\."), "Senate", "House")) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))

process %>% head()
  
```


```{r process for zoorob freq}

leg <- read_csv("legislators-current.csv") #member info file
key <- read_xlsx('key.xlsx') #key between names of member info and processed press release names
df <- read_rds('fullscrape.rds') #opioid press releases

#Process member info file and join to key
legkey <- leg %>% 
  mutate(full_name = str_to_lower(full_name)) %>%
  mutate(full_name = str_replace_all(full_name, 'á', 'a')) %>%
  mutate(full_name = str_replace_all(full_name, 'é', 'e')) %>%
  mutate(full_name = str_replace_all(full_name, 'ó', 'o')) %>%
  mutate(full_name = str_replace_all(full_name, 'í', 'i')) %>%
  mutate(full_name = str_replace_all(full_name, 'ú', 'u')) %>%
  mutate(full_name = str_replace_all(full_name, 'ñ', 'n')) %>%
  mutate(full_name = str_replace_all(full_name, ', jr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' jr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ', sr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' sr.', '')) %>%
  mutate(full_name = str_replace_all(full_name, ', iii', '')) %>%
  mutate(full_name = str_replace_all(full_name, ' iii', '')) %>%
  mutate(full_name = str_replace_all(full_name, "’", "'")) %>%
  left_join(key, by = "full_name")

#Process opioid dataframe and join to key
process <- df %>%
  filter(str_detect(date, "2019") | 
         str_detect(date, "Jan. 3, 2020") | 
         str_detect(date, "Jan. 2, 2020") | 
         str_detect(date, "Jan. 1, 2020")) %>%
  filter(!str_detect(date, "Jan. 2, 2019") & 
         !str_detect(date, "Jan. 1, 2019")) %>%
  mutate(year = NA,
         month = NA,
         day = NA) %>%
  mutate(year = ifelse(str_detect(date, "2019"), "2019", year),
         year = ifelse(str_detect(date, "2020"), "2020", year)) %>%
  mutate(month = ifelse(str_detect(date, "Jan"), "1", month),
         month = ifelse(str_detect(date, "Feb"), "2", month),
         month = ifelse(str_detect(date, "Mar"), "3", month),
         month = ifelse(str_detect(date, "Apr"), "4", month),
         month = ifelse(str_detect(date, "May"), "5", month),
         month = ifelse(str_detect(date, "Jun"), "6", month),
         month = ifelse(str_detect(date, "Jul"), "7", month),
         month = ifelse(str_detect(date, "Aug"), "8", month),
         month = ifelse(str_detect(date, "Sep"), "9", month),
         month = ifelse(str_detect(date, "Oct"), "10", month),
         month = ifelse(str_detect(date, "Nov"), "11", month),
         month = ifelse(str_detect(date, "Dec"), "12", month)) %>%
  mutate(day = str_extract(date, "[[:digit:]]+")) %>%
  mutate(text = str_remove_all(text, " All content © 1992 - 2020 Vote Smart unless otherwise attributed - Privacy Policy - Legislative demographic data provided by Aristotle International, Inc. Mobile Version \\#\\{text\\} You are about to be redirected to a secure checkout page. Please note: The total order amount will read \\$0.01. This is a card processor fee. Please know that a recurring donation of the amount and frequency that you selected will be processed and initiated tomorrow. You may see a one-time charge of \\$0.01 on your statement. Continue to secure page »")) %>%
  mutate(office = ifelse(str_detect(member, "Sen\\."), "Senate", "House")) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  mutate(member = str_replace_all(member, "’", "'")) %>%
  left_join(key, by = "member") %>%
  distinct()

legop <- left_join(process, legkey, by = c("member", "full_name"))
legop %>% write_csv("clean116.csv")
legop <- read_csv("clean116.csv")
legop %>% select(-text)

year <- read_rds('allstatements.rds') %>%
  filter(str_detect(date, "2019") | 
         str_detect(date, "Jan. 3, 2020") | 
         str_detect(date, "Jan. 2, 2020") | 
         str_detect(date, "Jan. 1, 2020")) %>%
  filter(!str_detect(date, "Jan. 2, 2019") & 
         !str_detect(date, "Jan. 1, 2019")) %>%
  mutate(year = NA,
         month = NA,
         day = NA) %>%
  mutate(year = ifelse(str_detect(date, "2019"), "2019", year),
         year = ifelse(str_detect(date, "2020"), "2020", year)) %>%
  mutate(month = ifelse(str_detect(date, "Jan"), "1", month),
         month = ifelse(str_detect(date, "Feb"), "2", month),
         month = ifelse(str_detect(date, "Mar"), "3", month),
         month = ifelse(str_detect(date, "Apr"), "4", month),
         month = ifelse(str_detect(date, "May"), "5", month),
         month = ifelse(str_detect(date, "Jun"), "6", month),
         month = ifelse(str_detect(date, "Jul"), "7", month),
         month = ifelse(str_detect(date, "Aug"), "8", month),
         month = ifelse(str_detect(date, "Sep"), "9", month),
         month = ifelse(str_detect(date, "Oct"), "10", month),
         month = ifelse(str_detect(date, "Nov"), "11", month),
         month = ifelse(str_detect(date, "Dec"), "12", month)) %>%
  mutate(day = str_extract(date, "[[:digit:]]+")) %>%
  mutate(office = ifelse(str_detect(member, "Sen\\."), "Senate", "House")) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  mutate(member = str_replace_all(member, "’", "'")) %>%
  left_join(key, by = "member") %>%
  distinct()

memcount <- year %>%
  filter(!is.na(date)) %>%
  distinct() %>%
  count(member) %>%
  arrange(desc(n)) %>%
  rename(total = n) %>%
  right_join(key) %>%
  mutate(total = ifelse(is.na(total), 0, total)) %>%
  arrange(total)

memcount %>% write_csv("memcount.csv")
read_csv("memcount.csv")


write_csv(legkey, "legkey.csv")
```

```{r build sentences dataset NOT THE CORRECT ONE; USE NEXT CHUNK}

legop <- read_csv("clean116.csv")

df <- legop %>%
  mutate(text = str_replace_all(text, "(?<=[:upper:])\\.", "")) %>% #this did not actually change anything in the opioid sentences
  mutate(text = str_replace_all(text, "(?<=[:alnum:])\\.(?=[:alnum:])", "")) %>% #neither did this
  mutate(text = str_replace_all(text, "Sen\\.", "Sen")) %>%
  mutate(text = str_replace_all(text, "Rep\\.", "Rep")) %>%
  mutate(text = str_replace_all(text, "Sens\\.", "Sens")) %>%
  mutate(text = str_replace_all(text, "Reps\\.", "Reps")) %>%
  mutate(text = str_replace_all(text, "Mrs\\.", "Mrs")) %>%
  mutate(text = str_replace_all(text, "Mr\\.", "Mr")) %>%
  mutate(text = str_replace_all(text, "Ms\\.", "Ms")) %>%
  mutate(text = str_replace_all(text, "Dr\\.", "Dr")) %>%
  mutate(text = str_replace_all(text, "Pres\\.", "Pres")) %>%
  mutate(text = str_replace_all(text, "St\\.", "St")) %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|oxycontin|codone|synthetic|morphine|codeine|methadone", ignore_case = TRUE))) %>% #Only 4 of the resulting sentences had a version of synthetic that was not about drugs
  mutate(sentence_id = row_number()) 

df2 <- df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)

hist(df2$n[df2$n > 10 & df2$n < 1000], breaks = 200)

a <- df %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)
hist(a$count, breaks = 1000)

#Separation if lowercase then $ immediately following
df3 <- df
for (row in 1:nrow(df3)) {
    if (str_detect(df3$text[row], '(?<=[:lower:])(?=\\$)') == TRUE) {
      df3$text[row] <- str_split(df3$text[row], '(?<=[:lower:])(?=\\$)')
      df3$text[row] <- paste(df3$text[row][[1]], collapse = ".")
      print(row)
    }
}

df4 <- df3 %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|synthetic|oxycontin|codone|morphine|codeine|methadone", ignore_case = TRUE))) %>%
  mutate(sentence_id = row_number())

df4 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)

df4 %>% filter(str_detect(text, '(?<=\\;)(?=[:graph:])'))

#Separation if lowercase before ;
df5 <- df4
for (row in 1:nrow(df5)) {
    if (str_detect(df5$text[row], '(?<=\\;)(?=[:graph:])') == TRUE) {
      df5$text[row] <- str_split(df5$text[row], '(?<=\\;)(?=[:graph:])')
      df5$text[row] <- paste(df5$text[row][[1]], collapse = ".")
      print(row)
    }
}

df6 <- df5 %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|synthetic|oxycontin|codone|morphine|codeine|methadone", ignore_case = TRUE))) %>%
  mutate(sentence_id = row_number())

df6 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)

#split weird bullet point character
df7 <- df6 %>%
  mutate(text = str_replace_all(text, "·", "."))  %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  mutate(sentence_id = row_number())


#split lower then upper immediately after if length greater than 99% of sentences
ct <- df7 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)

length(ct$count[ct$count > 793]) / length(ct$count)

df8 <- df7
for (row in 1:nrow(df8)) {
    if (str_detect(df8$text[row], '(?<=[:lower:])(?=[:upper:])') == TRUE &
        str_count(df8$text[row]) > 793) {
      df8$text[row] <- str_split(df8$text[row], '(?<=[:lower:])(?=[:upper:])')
      df8$text[row] <- paste(df8$text[row][[1]], collapse = ".")
      print(row)
    }
}

df9 <- df8 %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|synthetic|oxycontin|codone|morphine|codeine|methadone", ignore_case = TRUE))) %>%
  mutate(sentence_id = row_number())

cdf <- df9 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)

sent_proc <- df9

sent_proc %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)

```


```{r explore sentence levels THIS IS THE ONE ACTUALLY USED}

legop <- read_csv("clean116.csv")

df <- legop %>%
  mutate(text = str_replace_all(text, "(?<=[:upper:])\\.", "")) %>%
  mutate(text = str_replace_all(text, "(?<=[:alnum:])\\.(?=[:alnum:])", "")) %>%
  mutate(text = str_replace_all(text, "Sen\\.", "Sen")) %>%
  mutate(text = str_replace_all(text, "Rep\\.", "Rep")) %>%
  mutate(text = str_replace_all(text, "Sens\\.", "Sens")) %>%
  mutate(text = str_replace_all(text, "Reps\\.", "Reps")) %>%
  mutate(text = str_replace_all(text, "Mrs\\.", "Mrs")) %>%
  mutate(text = str_replace_all(text, "Mr\\.", "Mr")) %>%
  mutate(text = str_replace_all(text, "Ms\\.", "Ms")) %>%
  mutate(text = str_replace_all(text, "Dr\\.", "Dr")) %>%
  mutate(text = str_replace_all(text, "Pres\\.", "Pres")) %>%
  mutate(text = str_replace_all(text, "St\\.", "St")) %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|narcan|synthetics|oxycodone|hydrocodone|morphine|codeine|methadone|meperidine|buprenorphine|hydromorphone|benzo", ignore_case = TRUE))) %>%
  mutate(sentence_id = row_number())

df %>% nrow()
a <- df %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)
hist(a$count, breaks = 1000)

#Separation if lowercase then $ immediately following
df3 <- df
for (row in 1:nrow(df3)) {
    if (str_detect(df3$text[row], '(?<=[:lower:])(?=\\$)') == TRUE) {
      df3$text[row] <- str_split(df3$text[row], '(?<=[:lower:])(?=\\$)')
      df3$text[row] <- paste(df3$text[row][[1]], collapse = ".")
      print(row)
    }
}

#Separation if lowercase before ;
for (row in 1:nrow(df3)) {
    if (str_detect(df3$text[row], '(?<=\\;)(?=[:graph:])') == TRUE) {
      df3$text[row] <- str_split(df3$text[row], '(?<=\\;)(?=[:graph:])')
      df3$text[row] <- paste(df3$text[row][[1]], collapse = ".")
      print(row)
    }
}

#replace weird bullet point character
df3 <- df3 %>%
  mutate(text = str_replace_all(text, "·", "."))  %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  mutate(sentence_id = row_number())

#split lower then upper immediately after if length greater than 99% of sentences
ct <- df3 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)

length(ct$count[ct$count > 793]) / length(ct$count)

for (row in 1:nrow(df3)) {
    if (str_detect(df3$text[row], '(?<=[:lower:])(?=[:upper:])') == TRUE &
        str_count(df3$text[row]) > 793) {
      df3$text[row] <- str_split(df3$text[row], '(?<=[:lower:])(?=[:upper:])')
      df3$text[row] <- paste(df3$text[row][[1]], collapse = ".")
      print(row)
    }
}

df4 <- df3 %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|narcan|synthetics|oxycodone|hydrocodone|morphine|codeine|methadone|meperidine|buprenorphine|hydromorphone|benzo", ignore_case = TRUE))) %>%
  mutate(sentence_id = row_number())

cdf <- df4 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)

hist(cdf$count, breaks = 1000)

sent_proc <- df4

#Words list
n1 <- sent_proc %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)

sum(n1$n)
corp_n1 <- n1 %>% filter(n > 85)

#Bigrams list
n2 <- sent_proc %>%
  mutate(text = str_to_lower(text)) %>%
  unnest_tokens(word, text, token = "ngrams", n = 2) %>%
  filter(!str_detect(word, paste0("^", paste0(stop_words$word, collapse = "[:space:]|^"), "[:space:]"))) %>%
  filter(!str_detect(word, paste0("[:space:]", paste0(stop_words$word, collapse = "$|[:space:]"), "$"))) %>%
  count(word, sort = TRUE)

use_bi <- n2 %>%
  filter(
    (!str_detect(word, paste0("^", paste0(corp_n1$word, collapse = "[:space:]|^"), "[:space:]")) & 
     !str_detect(word, paste0("[:space:]", paste0(corp_n1$word, collapse = "$|[:space:]"), "$"))) |
     (str_detect(word, paste0("^", paste0(corp_n1$word, collapse = "[:space:]|^"), "[:space:]")) & 
     str_detect(word, paste0("[:space:]", paste0(corp_n1$word, collapse = "$|[:space:]"), "$"))) 
    )

corp_n2 <- use_bi %>% filter(n>85)


#Trigrams list
tres <- sent_proc %>%
  mutate(text = str_to_lower(text)) %>%
  unnest_tokens(word, text, token = "ngrams", n = 3) %>%
  filter(!str_detect(word, paste0("^", paste0(stop_words$word, collapse = "[:space:]|^"), "[:space:]"))) %>%
  filter(!str_detect(word, paste0("[:space:]", paste0(stop_words$word, collapse = "$|[:space:]"), "$"))) %>%
  count(word, sort = TRUE)

corpus <- tres %>% 
  filter(word == 'medication assisted treatment') %>%
  rbind(corp_n2) %>%
  rbind(corp_n1)

write_csv(corpus, "corpus116.csv")
write_csv(sent_proc, "sentences.csv")

sent_proc <- read_csv("sentences.csv")

###WHICH MEMBERS ARE STILL IN THIS SAMPLE
read_csv("clean116.csv") %>% count(member) %>% nrow()
df %>% count(member) %>% nrow()
sent_proc %>% count(member) %>% nrow()

a <- read_csv("clean116.csv") %>% count(member)
b <- sent_proc %>% count(member)
anti_join(a, b, by = "member")

#These people are no longer in dataset when split by sentences because only PDF available for these releases (all of these people only had one press release)
read_csv("clean116.csv") %>% filter(member == "abby finkenauer")
read_csv("clean116.csv") %>% filter(member == "andre carson")
read_csv("clean116.csv") %>% filter(member == "doris matsui")
read_csv("clean116.csv") %>% filter(member == "ilhan omar")
read_csv("clean116.csv") %>% filter(member == "jason crow")
read_csv("clean116.csv") %>% filter(member == "scott peters")
read_csv("clean116.csv") %>% filter(member == "sylvia garcia")
read_csv("clean116.csv") %>% filter(member == "veronica escobar")

#These people are no longer in dataset when split by sentences because their only press release implicitly discussed drug epidemics but never mentioned anything using opioid language explicitly (all of these people only had one press release)
kev <- read_csv("clean116.csv") %>% filter(member == "kevin cramer")
read_csv("clean116.csv") %>% filter(member == "marion rounds")


```

```{r member scoring}

sent_proc <- read_csv("sentences.csv")
sample(sent_proc$text, 30)


#399 grams in corpus; 78 informative ngrams; 38 public health (1), 40 law enforcement (-1)
dict1 <- read_xlsx("Dictionary_use.xlsx") %>%
  rename(value = `Final Code`, word = Term) %>%
  filter(value != 0)

#27 informative bigram, 1 informative trigram; only 10 with values of 1 or -1 for scoring
grams <- read_xlsx("ngrams_dictionary.xlsx") %>%
  rename(ngram = `Ngram Code`, value = `Final Code`, word = Term) %>%
  select(-Notes) %>%
  filter(ngram != 0)

dict1 <- dict1 %>%
  mutate(gram = 1) %>%
  mutate(gram = ifelse(str_detect(word, " "), 2, gram)) %>%
  mutate(gram = ifelse(str_detect(word, "medication assisted treatment"), 3, gram))

grams <- grams %>%
  mutate(ngram = value) %>%
  select(-ngram)
  
#SCORING ONLY USING 1-GRAMS
sent_id <- sent_proc %>%
  mutate(sentence_id = row_number()) 

sent_scores <- sent_id %>%
  group_by(sentence_id) %>%
  unnest_tokens(word, text) %>%
  inner_join(dict1) %>%
  ungroup() %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(value)) %>%
  right_join(sent_id) %>%
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

hist(sent_scores$sent_score)

mem_scores <- sent_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

hist(mem_scores$mem_score, breaks = 100)

mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

#3863 included no informative words
 na <- sent_id %>%
   group_by(sentence_id) %>%
   unnest_tokens(word, text) %>%
   anti_join(stop_words) %>%
   inner_join(dict) %>%
   ungroup() %>%
   group_by(sentence_id) %>%
   summarise(sent_score = mean(value)) %>%
   right_join(sent_id) %>%
   filter(is.na(sent_score)) 
 
 sample(na$text, 50)

```

```{r many member scoring schemes}

#Load full dictionary
dict <- read_xlsx("Dictionary_ALL.xlsx") %>%
  rename(value = `Final Value`, word = Term, code = Code) %>%
  #GOTTA DO THIS STEP
  filter(code != 0)

#Build dataset with trigrams, bigrams, and unigrams of each sentence

sent_n1 <- sent_id %>%
  group_by(sentence_id) %>%
  unnest_tokens(word, text)

sent_n2 <- sent_id %>%
  group_by(sentence_id) %>%
  unnest_tokens(word, text, token = "ngrams", n = 2)

sent_n3 <- sent_id %>%
  group_by(sentence_id) %>%
  unnest_tokens(word, text, token = "ngrams", n = 3)

sent_gram <- rbind(sent_n3, sent_n2, sent_n1)

#1) Full Dictionary; NA Sentences = 0

dict_all <- dict %>%
  filter(value != 0)

sent_gram_scores <- sent_gram %>%
  ungroup() %>%
  inner_join(dict_all) %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(value)) %>%
  right_join(sent_id) %>%
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_full0.rds")

gram_mem_scores <- sent_gram_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_full0.rds")


#2) Full Dictionary; NA Sentences = Removed

dict_all <- dict %>%
  filter(value != 0)

sent_gram_scores <- sent_gram %>%
  ungroup() %>%
  inner_join(dict_all) %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(value)) %>%
  right_join(sent_id) %>%
  filter(!is.na(sent_score))

write_rds(sent_gram_scores, "sentences_fullNA.rds")

gram_mem_scores <- sent_gram_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  filter(!is.na(mem_score)) %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_fullNA.rds")


#3) Public Health Dictionary

dict_health <- dict %>%
  filter(code == 1)
  
sent_gram_scores <- sent_gram %>%
  ungroup() %>%
  inner_join(dict_health) %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(code)) %>%
  right_join(sent_id) %>%
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_health.rds")

gram_mem_scores <- sent_gram_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  filter(!is.na(mem_score)) %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_health.rds")


#4) Law Enforcement Dictionary

dict_law <- dict %>%
  filter(code == -1)
  
sent_gram_scores <- sent_gram %>%
  ungroup() %>%
  inner_join(dict_law) %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(code)) %>%
  right_join(sent_id) %>%
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_law.rds")

gram_mem_scores <- sent_gram_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  filter(!is.na(mem_score)) %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_law.rds")


#5) Only Unigrams; NA Sentences = 0

dict_all <- dict %>%
  filter(value != 0)
  
sent_gram_scores <- sent_n1 %>%
  ungroup() %>%
  inner_join(dict_all) %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(value)) %>%
  right_join(sent_id) %>%
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_full0uni.rds")

gram_mem_scores <- sent_gram_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_full0uni.rds")


#6) Only Unigrams; NA Sentences = Removed

dict_all <- dict %>%
  filter(value != 0)
  
sent_gram_scores <- sent_n1 %>%
  ungroup() %>%
  inner_join(dict_all) %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(value)) %>%
  right_join(sent_id) %>%
  filter(!is.na(sent_score))

write_rds(sent_gram_scores, "sentences_fullNAuni.rds")

gram_mem_scores <- sent_gram_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  filter(!is.na(mem_score)) %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_fullNAuni.rds")


#Have a party
hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

mean(gram_mem_scores$mem_score)
```



```{r key word count exploration}

#Talking about foreigners
dict_foreign <- read_xlsx("dict_foreign.xlsx") %>%
  filter(foreign == 1) %>%
  rename(code = foreign)
  
sent_gram_scores <- sent_gram %>%
  ungroup() %>%
  inner_join(dict_foreign) %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(code)) %>%
  right_join(sent_id) %>%
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_foreign.rds")

 gram_mem_scores <- sent_gram_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  filter(!is.na(mem_score)) %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_foreign.rds")


#Talking about MAT
dict_foreign <- read_xlsx("dict_MAT.xlsx") %>%
  filter(MAT == 1) %>%
  rename(code = MAT)
  
sent_gram_scores <- sent_gram %>%
  ungroup() %>%
  inner_join(dict_foreign) %>%
  group_by(sentence_id) %>%
  summarise(sent_score = mean(code)) %>%
  right_join(sent_id) %>%
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_MAT.rds")

gram_mem_scores <- sent_gram_scores %>%
  group_by(member) %>%
  summarise(mem_score = mean(sent_score)) %>%
  right_join(sent_id, by = "member") %>%
  filter(!is.na(mem_score)) %>%
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_MAT.rds")

gram_mem_scores %>% arrange(desc(mem_score))


#Talking about MAT
sent_gram_scores <- sent_gram %>%
  ungroup() %>%
  filter(word %in% c("addiction", "misuse", "abuse", "disorder", "disease")) %>%
  group_by(sentence_id) %>%
  count(word) %>%
  spread(word, n) %>%
  right_join(sent_id) %>%
  mutate(abuse = ifelse(is.na(abuse), 0, abuse),
         misuse = ifelse(is.na(misuse), 0, misuse),
         disorder = ifelse(is.na(disorder), 0, disorder),
         addiction = ifelse(is.na(addiction), 0, addiction),
         disease = ifelse(is.na(disease), 0, disease))

write_rds(sent_gram_scores, "sentences_disorder.rds")

#Have a party
hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

mean(gram_mem_scores$mem_score)
```


```{r fixed scoring}

#Load full dictionary
dict_all <- read_xlsx("Dictionary_ALL.xlsx") %>%
  rename(value = `Final Value`, word = Term, code = Code) %>%
  select(-value) %>%
#Remove uninformative ngrams
  filter(code != 0)

#Load in sentences dataset
sent_proc <- read_csv("sentences.csv")
#Add sentence id numbers
sent_id <- sent_proc %>%
  mutate(sentence_id = row_number()) 

#three has all of the trigrams from a sentence that matched one of the trigrams in the dictionary
three <- sent_id %>%
  unnest_tokens(word, text, token = "ngrams", n = 3) %>%
  inner_join(dict_all)

#Build sentaur3, which has the same columns as sent_id but just one row of NAs
sentaur3 <- sent_id
sentaur3[nrow(sentaur3)+1,] <- NA
sentaur3 <- sentaur3 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id)){
  #Filter matched trigrams to only those for the given sentence id
  a <- three %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>% 
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any trigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the trigram matches
    for(j in 1:nrow(a)){
    #Print trigram match
    print(a[j,]$word)
    #replace the trigram match from the given text with the word TRIGRAM
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "trigram"))
    }
    #Add to the running dataframe
    sentaur3 <- rbind(sentaur3, b)
  }
}

#Bind the sentences where matched trigrams replaced with TRIGRAM to the original sentence set
sent_id3 <- rbind(sentaur3, sent_id) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the TRIGRAM replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#two has all of the bigrams from a sentence that matched one of the bigrams in the dictionary
two <- sent_id3 %>%
  unnest_tokens(word, text, token = "ngrams", n = 2) %>%
  inner_join(dict_all) 

#Build sentaur2, which has the same columns as sent_id3 (and sent_id) but just one row of NAs
sentaur2 <- sent_id3
sentaur2[nrow(sentaur2)+1,] <- NA
sentaur2 <- sentaur2 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id3)){
  #Filter matched bigrams to only those for the given sentence id
  a <- two %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id3 %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>%
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any bigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the bigram matches
    for(j in 1:nrow(a)){
    #Print bigram match
    print(a[j,]$word)
    #remove the bigram match from the given text with the word BIGRAM
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "bigram"))
    }
    #Add to the running dataframe
    sentaur2 <- rbind(sentaur2, b)
  }
}

#Bind the sentences where matched bigrams replaced with BIGRAM to the set of distinct sentences that have TRIGRAM replacements where relevant
sent_id2 <- rbind(sentaur2, sent_id3) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the TRIGRAM replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#one has all of the unigrams from a sentence that matched one of the unigrams in the dictionary
one <- sent_id2 %>%
  unnest_tokens(word, text) %>%
  inner_join(dict_all)

#Sentence and Member Scoring -  sentences with no informative terms (NA) represented as arithmetic 0
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them sentence scores
  right_join(sent_id) %>%
  #Replace NA scores with 0
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_full0.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  #Only keep one of the duplicated rows for each member
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_full0.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

#Sentence and Member Scoring -  sentences with no informative terms (NA) removed
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them scores where relevant (i.e. not NA)
  right_join(sent_id) %>%
  #Remove all scores of NA
  filter(!is.na(sent_score))

write_rds(sent_gram_scores, "sentences_fullNA.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  #Only keep one of the duplicated rows for each member
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_fullNA.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))
z <- read_rds("sentences_fullNA.rds")

y <- z %>% filter(sentence_id == 9123)
rbind(three, two, one) %>% filter(sentence_id == 9123)
y$text

sent_id3 %>% filter(sentence_id == 8993)
```

```{r working scoring but for press releases}

#Load full dictionary
dict_all <- read_xlsx("Dictionary_ALL.xlsx") %>%
  rename(value = `Final Value`, word = Term, code = Code) %>%
  select(-value) %>%
#Remove uninformative ngrams
  filter(code != 0)

#Load in press releases dataset
legop <- read_csv("clean116.csv")
sent_proc <- legop
#Add sentence id numbers
sent_id <- sent_proc %>%
  mutate(sentence_id = row_number()) 

#three has all of the trigrams from a sentence that matched one of the trigrams in the dictionary
three <- sent_id %>%
  unnest_tokens(word, text, token = "ngrams", n = 3) %>%
  inner_join(dict_all)

#Build sentaur3, which has the same columns as sent_id but just one row of NAs
sentaur3 <- sent_id
sentaur3[nrow(sentaur3)+1,] <- NA
sentaur3 <- sentaur3 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id)){
  #Filter matched trigrams to only those for the given sentence id
  a <- three %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>% 
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any trigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the trigram matches
    for(j in 1:nrow(a)){
    #Print trigram match
    print(a[j,]$word)
    #replace the trigram match from the given text with the word TRIGRAM
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "trigram"))
    }
    #Add to the running dataframe
    sentaur3 <- rbind(sentaur3, b)
  }
}

#Bind the sentences where matched trigrams replaced with TRIGRAM to the original sentence set
sent_id3 <- rbind(sentaur3, sent_id) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the TRIGRAM replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#two has all of the bigrams from a sentence that matched one of the bigrams in the dictionary
two <- sent_id3 %>%
  unnest_tokens(word, text, token = "ngrams", n = 2) %>%
  inner_join(dict_all) 

#Build sentaur2, which has the same columns as sent_id3 (and sent_id) but just one row of NAs
sentaur2 <- sent_id3
sentaur2[nrow(sentaur2)+1,] <- NA
sentaur2 <- sentaur2 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id3)){
  #Filter matched bigrams to only those for the given sentence id
  a <- two %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id3 %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>%
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any bigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the bigram matches
    for(j in 1:nrow(a)){
    #Print bigram match
    print(a[j,]$word)
    #remove the bigram match from the given text with the word BIGRAM
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "bigram"))
    }
    #Add to the running dataframe
    sentaur2 <- rbind(sentaur2, b)
  }
}

#Bind the sentences where matched bigrams replaced with BIGRAM to the set of distinct sentences that have TRIGRAM replacements where relevant
sent_id2 <- rbind(sentaur2, sent_id3) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the TRIGRAM replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#one has all of the unigrams from a sentence that matched one of the unigrams in the dictionary
one <- sent_id2 %>%
  unnest_tokens(word, text) %>%
  inner_join(dict_all)

#Sentence and Member Scoring -  sentences with no informative terms (NA) represented as arithmetic 0
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them sentence scores
  right_join(sent_id) %>%
  #Replace NA scores with 0
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "PR_full0.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  #Only keep one of the duplicated rows for each member
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "PRmembers_full0.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

#Sentence and Member Scoring -  sentences with no informative terms (NA) removed
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them scores where relevant (i.e. not NA)
  right_join(sent_id) %>%
  #Remove all scores of NA
  filter(!is.na(sent_score))

write_rds(sent_gram_scores, "PR_fullNA.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  #Only keep one of the duplicated rows for each member
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "PRmembers_fullNA.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))
 

```

```{r full scrapes freqs}
###FOR MEMBER-YEAR combo
allstatements <- read_rds('allstatements.rds') %>%
  filter(!is.na(member)) %>%
  filter(date != "Feb. 18, 2020" &
         date != "Feb. 19, 2020" &
         date != "Feb. 20, 2020" &
         date != "Feb. 21, 2020" & 
         date != "Feb. 22, 2020" &
         date != "Feb. 23, 2020" &
         date != "Feb. 24, 2020" &           
         date != "Feb. 25, 2020" &           
         date != "Feb. 26, 2020") #start on feb 17        
                      
allstatements1 <- allstatements %>%
  mutate(year = as.character(date)) %>%
  mutate(year = ifelse(str_detect(year, "1961"), "1961", year),
         year = ifelse(str_detect(year, "1996"), "1996", year),
         year = ifelse(str_detect(year, "1997"), "1997", year),
         year = ifelse(str_detect(year, "1998"), "1998", year),
         year = ifelse(str_detect(year, "1999"), "1999", year),
         year = ifelse(str_detect(year, "2000"), "2000", year),
         year = ifelse(str_detect(year, "2001"), "2001", year),
         year = ifelse(str_detect(year, "2002"), "2002", year),
         year = ifelse(str_detect(year, "2003"), "2003", year),
         year = ifelse(str_detect(year, "2004"), "2004", year),
         year = ifelse(str_detect(year, "2005"), "2005", year),
         year = ifelse(str_detect(year, "2006"), "2006", year),
         year = ifelse(str_detect(year, "2007"), "2007", year),
         year = ifelse(str_detect(year, "2008"), "2008", year),
         year = ifelse(str_detect(year, "2009"), "2009", year),
         year = ifelse(str_detect(year, "2010"), "2010", year),
         year = ifelse(str_detect(year, "2011"), "2011", year),
         year = ifelse(str_detect(year, "2012"), "2012", year),
         year = ifelse(str_detect(year, "2013"), "2013", year),
         year = ifelse(str_detect(year, "2014"), "2014", year),
         year = ifelse(str_detect(year, "2015"), "2015", year),
         year = ifelse(str_detect(year, "2016"), "2016", year),
         year = ifelse(str_detect(year, "2017"), "2017", year),
         year = ifelse(str_detect(year, "2018"), "2018", year),
         year = ifelse(str_detect(year, "2019"), "2019", year),
         year = ifelse(str_detect(year, "2020"), "2020", year)) %>%
  count(member, year) %>%
  arrange(desc(n)) %>%
  rename(total = n)

opioid <- read_rds('fullscrape.rds')

opioid1 <- opioid %>%
  filter(!is.na(member)) %>%
  filter(date != "Feb. 18, 2020") %>% #start on feb 17
  mutate(year = as.character(date)) %>%
  mutate(year = ifelse(str_detect(year, "1961"), "1961", year),
         year = ifelse(str_detect(year, "1996"), "1996", year),
         year = ifelse(str_detect(year, "1997"), "1997", year),
         year = ifelse(str_detect(year, "1998"), "1998", year),
         year = ifelse(str_detect(year, "1999"), "1999", year),
         year = ifelse(str_detect(year, "2000"), "2000", year),
         year = ifelse(str_detect(year, "2001"), "2001", year),
         year = ifelse(str_detect(year, "2002"), "2002", year),
         year = ifelse(str_detect(year, "2003"), "2003", year),
         year = ifelse(str_detect(year, "2004"), "2004", year),
         year = ifelse(str_detect(year, "2005"), "2005", year),
         year = ifelse(str_detect(year, "2006"), "2006", year),
         year = ifelse(str_detect(year, "2007"), "2007", year),
         year = ifelse(str_detect(year, "2008"), "2008", year),
         year = ifelse(str_detect(year, "2009"), "2009", year),
         year = ifelse(str_detect(year, "2010"), "2010", year),
         year = ifelse(str_detect(year, "2011"), "2011", year),
         year = ifelse(str_detect(year, "2012"), "2012", year),
         year = ifelse(str_detect(year, "2013"), "2013", year),
         year = ifelse(str_detect(year, "2014"), "2014", year),
         year = ifelse(str_detect(year, "2015"), "2015", year),
         year = ifelse(str_detect(year, "2016"), "2016", year),
         year = ifelse(str_detect(year, "2017"), "2017", year),
         year = ifelse(str_detect(year, "2018"), "2018", year),
         year = ifelse(str_detect(year, "2019"), "2019", year),
         year = ifelse(str_detect(year, "2020"), "2020", year)) %>%
  count(member, year) %>%
  arrange(desc(n)) %>%
  rename(opioid = n)

memcounts <- left_join(allstatements1, opioid1) %>%
  mutate(opioid = ifelse(is.na(opioid), 0, opioid)) %>%
  mutate(prop = opioid / total) %>%
  arrange(desc(prop))

legkey <- read_csv("legkey.csv")

memcounts_indexed <- memcounts %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  mutate(member = str_replace_all(member, "’", "'")) %>%
  left_join(legkey, by = "member")

write_rds(memcounts_indexed, "freq_allvotesmart.rds")



###FOR JUST MEMBER

allstatements <- read_rds('allstatements.rds') %>%
  filter(!is.na(member)) %>%
  filter(date != "Feb. 18, 2020" &
         date != "Feb. 19, 2020" &
         date != "Feb. 20, 2020" &
         date != "Feb. 21, 2020" & 
         date != "Feb. 22, 2020" &
         date != "Feb. 23, 2020" &
         date != "Feb. 24, 2020" &           
         date != "Feb. 25, 2020" &           
         date != "Feb. 26, 2020") #start on feb 17   
  
                      
allstatements1 <- allstatements %>%
  count(member) %>%
  arrange(desc(n)) %>%
  rename(total = n)
  
opioid <- read_rds('fullscrape.rds')

opioid1 <- opioid %>%
  filter(!is.na(member)) %>%
  filter(date != "Feb. 18, 2020") %>% #start on feb 17
  count(member) %>%
  arrange(desc(n)) %>%
  rename(opioid = n)

memcounts <- left_join(allstatements1, opioid1) %>%
  mutate(opioid = ifelse(is.na(opioid), 0, opioid)) %>%
  mutate(prop = opioid / total) %>%
  arrange(desc(prop))

legkey <- read_csv("legkey.csv")

memcounts_indexed <- memcounts %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  mutate(member = str_replace_all(member, "’", "'")) %>%
  left_join(legkey, by = "member")

write_rds(memcounts_indexed, "freq_allvotesmart_memberyear.rds")

#Make datasets of all statements with metadata for dropbox
read_rds('allstatements.rds') %>%
  filter(!is.na(member)) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  mutate(member = str_replace_all(member, "’", "'")) %>%
  left_join(legkey, by = "member") %>%
  write_rds("metadata_allvotesmart.rds")
  
read_rds('fullscrape.rds') %>%
  filter(!is.na(member)) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  mutate(member = str_replace_all(member, "’", "'")) %>%
  left_join(legkey, by = "member") %>%
  write_rds("opioids_allvotesmart.rds")
```

```{r score full scrape (not just 116th)}

opioid <- read_rds('fullscrape.rds')
legkey <- read_csv("legkey.csv")

opioid1 <- opioid %>%
  filter(!is.na(member)) %>%
  filter(date != "Feb. 18, 2020") %>% #start on feb 17
  filter(!is.na(member)) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  mutate(member = str_replace_all(member, "’", "'")) %>%
  left_join(legkey, by = "member")

opioid1


df <- opioid1 %>%
  mutate(text = str_replace_all(text, "(?<=[:upper:])\\.", "")) %>%
  mutate(text = str_replace_all(text, "(?<=[:alnum:])\\.(?=[:alnum:])", "")) %>%
  mutate(text = str_replace_all(text, "Sen\\.", "Sen")) %>%
  mutate(text = str_replace_all(text, "Rep\\.", "Rep")) %>%
  mutate(text = str_replace_all(text, "Sens\\.", "Sens")) %>%
  mutate(text = str_replace_all(text, "Reps\\.", "Reps")) %>%
  mutate(text = str_replace_all(text, "Mrs\\.", "Mrs")) %>%
  mutate(text = str_replace_all(text, "Mr\\.", "Mr")) %>%
  mutate(text = str_replace_all(text, "Ms\\.", "Ms")) %>%
  mutate(text = str_replace_all(text, "Dr\\.", "Dr")) %>%
  mutate(text = str_replace_all(text, "Pres\\.", "Pres")) %>%
  mutate(text = str_replace_all(text, "St\\.", "St")) %>%
  mutate(text = str_replace_all(text, "U\\.S\\.", "US")) %>%
  mutate(text = str_replace_all(text, "U\\.S\\.A\\.", "USA")) %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|narcan|synthetics|oxycodone|hydrocodone|morphine|codeine|methadone|meperidine|buprenorphine|hydromorphone|benzo", ignore_case = TRUE))) %>%
  mutate(sentence_id = row_number())
#replace weird bullet point character
df2 <- df %>%
  mutate(text = str_replace_all(text, "·", "."))  %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  mutate(sentence_id = row_number())

df2 %>% nrow() #64089 rows

a <- df2 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)
hist(a$count, breaks = 1000)

boxplot(a$count)$out
#24 and fewer characters should be removed (it is when they stop making sense as individual sentences and weird stuff must have happened)
#415 and greater are seen as outliers

b <- df2 %>%
  mutate(count = str_count(text)) %>%
  filter(count > 24 & count < 415)
b %>% nrow() #61128 rows, 4.62% removed

write_rds(b, "sentences_allvotesmart.rds")

###SCORE SCORE SCORE###

#Load full dictionary
dict_all <- read_xlsx("Dictionary_ALL.xlsx") %>%
  rename(value = `Final Value`, word = Term, code = Code) %>%
  select(-value) %>%
#Remove uninformative ngrams
  filter(code != 0)

#Load in sentences dataset
sent_proc <- b
#Add sentence id numbers
sent_id <- sent_proc %>%
  mutate(sentence_id = row_number()) 

#three has all of the trigrams from a sentence that matched one of the trigrams in the dictionary
three <- sent_id %>%
  unnest_tokens(word, text, token = "ngrams", n = 3) %>%
  inner_join(dict_all)

#Build sentaur3, which has the same columns as sent_id but just one row of NAs
sentaur3 <- sent_id
sentaur3[nrow(sentaur3)+1,] <- NA
sentaur3 <- sentaur3 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id)){
  #Filter matched trigrams to only those for the given sentence id
  a <- three %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>% 
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any trigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the trigram matches
    for(j in 1:nrow(a)){
    #Print trigram match
    print(a[j,]$word)
    #replace the trigram match from the given text with the word TRIGRAM
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "trigram"))
    }
    #Add to the running dataframe
    sentaur3 <- rbind(sentaur3, b)
  }
}

#Bind the sentences where matched trigrams replaced with TRIGRAM to the original sentence set
sent_id3 <- rbind(sentaur3, sent_id) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the TRIGRAM replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#two has all of the bigrams from a sentence that matched one of the bigrams in the dictionary
two <- sent_id3 %>%
  unnest_tokens(word, text, token = "ngrams", n = 2) %>%
  inner_join(dict_all) 

#Build sentaur2, which has the same columns as sent_id3 (and sent_id) but just one row of NAs
sentaur2 <- sent_id3
sentaur2[nrow(sentaur2)+1,] <- NA
sentaur2 <- sentaur2 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id3)){
  #Filter matched bigrams to only those for the given sentence id
  a <- two %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id3 %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>%
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any bigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the bigram matches
    for(j in 1:nrow(a)){
    #Print bigram match
    print(a[j,]$word)
    #remove the bigram match from the given text with the word BIGRAM
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "bigram"))
    }
    #Add to the running dataframe
    sentaur2 <- rbind(sentaur2, b)
  }
}

#Bind the sentences where matched bigrams replaced with BIGRAM to the set of distinct sentences that have TRIGRAM replacements where relevant
sent_id2 <- rbind(sentaur2, sent_id3) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the TRIGRAM replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#one has all of the unigrams from a sentence that matched one of the unigrams in the dictionary
one <- sent_id2 %>%
  unnest_tokens(word, text) %>%
  inner_join(dict_all)

#Sentence and Member Scoring -  sentences with no informative terms (NA) represented as arithmetic 0
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them sentence scores
  right_join(sent_id) %>%
  #Replace NA scores with 0
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_full0_ALL.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -sentence_id) %>%
  #Only keep one of the duplicated rows for each member
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_full0_ALL.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

#Sentence and Member Scoring -  sentences with no informative terms (NA) removed
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them scores where relevant (i.e. not NA)
  right_join(sent_id) %>%
  #Remove all scores of NA
  filter(!is.na(sent_score))

write_rds(sent_gram_scores, "sentences_fullNA_ALL.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -sentence_id) %>%
  #Only keep one of the duplicated rows for each member
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_fullNA_ALL.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

####SAME THING BUT FOR MEMBER-YEAR instead of just member######
opioid <- read_rds('fullscrape.rds')
legkey <- read_csv("legkey.csv")

opioid1 <- opioid %>%
  filter(!is.na(member)) %>%
  filter(date != "Feb. 18, 2020") %>% #start on feb 17
  mutate(year = as.character(date)) %>%
  mutate(year = ifelse(str_detect(year, "1961"), "1961", year),
         year = ifelse(str_detect(year, "1996"), "1996", year),
         year = ifelse(str_detect(year, "1997"), "1997", year),
         year = ifelse(str_detect(year, "1998"), "1998", year),
         year = ifelse(str_detect(year, "1999"), "1999", year),
         year = ifelse(str_detect(year, "2000"), "2000", year),
         year = ifelse(str_detect(year, "2001"), "2001", year),
         year = ifelse(str_detect(year, "2002"), "2002", year),
         year = ifelse(str_detect(year, "2003"), "2003", year),
         year = ifelse(str_detect(year, "2004"), "2004", year),
         year = ifelse(str_detect(year, "2005"), "2005", year),
         year = ifelse(str_detect(year, "2006"), "2006", year),
         year = ifelse(str_detect(year, "2007"), "2007", year),
         year = ifelse(str_detect(year, "2008"), "2008", year),
         year = ifelse(str_detect(year, "2009"), "2009", year),
         year = ifelse(str_detect(year, "2010"), "2010", year),
         year = ifelse(str_detect(year, "2011"), "2011", year),
         year = ifelse(str_detect(year, "2012"), "2012", year),
         year = ifelse(str_detect(year, "2013"), "2013", year),
         year = ifelse(str_detect(year, "2014"), "2014", year),
         year = ifelse(str_detect(year, "2015"), "2015", year),
         year = ifelse(str_detect(year, "2016"), "2016", year),
         year = ifelse(str_detect(year, "2017"), "2017", year),
         year = ifelse(str_detect(year, "2018"), "2018", year),
         year = ifelse(str_detect(year, "2019"), "2019", year),
         year = ifelse(str_detect(year, "2020"), "2020", year)) %>%
  filter(!is.na(member)) %>%
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  mutate(member = str_to_lower(member)) %>%
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  mutate(member = str_replace_all(member, "’", "'")) %>%
  left_join(legkey, by = "member")

df <- opioid1 %>%
  mutate(text = str_replace_all(text, "(?<=[:upper:])\\.", "")) %>%
  mutate(text = str_replace_all(text, "(?<=[:alnum:])\\.(?=[:alnum:])", "")) %>%
  mutate(text = str_replace_all(text, "Sen\\.", "Sen")) %>%
  mutate(text = str_replace_all(text, "Rep\\.", "Rep")) %>%
  mutate(text = str_replace_all(text, "Sens\\.", "Sens")) %>%
  mutate(text = str_replace_all(text, "Reps\\.", "Reps")) %>%
  mutate(text = str_replace_all(text, "Mrs\\.", "Mrs")) %>%
  mutate(text = str_replace_all(text, "Mr\\.", "Mr")) %>%
  mutate(text = str_replace_all(text, "Ms\\.", "Ms")) %>%
  mutate(text = str_replace_all(text, "Dr\\.", "Dr")) %>%
  mutate(text = str_replace_all(text, "Pres\\.", "Pres")) %>%
  mutate(text = str_replace_all(text, "St\\.", "St")) %>%
  mutate(text = str_replace_all(text, "U\\.S\\.", "US")) %>%
  mutate(text = str_replace_all(text, "U\\.S\\.A\\.", "USA")) %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|narcan|synthetics|oxycodone|hydrocodone|morphine|codeine|methadone|meperidine|buprenorphine|hydromorphone|benzo", ignore_case = TRUE))) %>%
  mutate(sentence_id = row_number())
#replace weird bullet point character
df2 <- df %>%
  mutate(text = str_replace_all(text, "·", "."))  %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  mutate(sentence_id = row_number())

df2 %>% nrow() #64089 rows

a <- df2 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)
hist(a$count, breaks = 1000)

boxplot(a$count)$out
#24 and fewer characters should be removed (it is when they stop making sense as individual sentences and weird stuff must have happened)
#415 and greater are seen as outliers

b <- df2 %>%
  mutate(count = str_count(text)) %>%
  filter(count > 24 & count < 415)
b %>% nrow() #61128 rows, 4.62% removed

###SCORE SCORE SCORE###

#Load full dictionary
dict_all <- read_xlsx("Dictionary_ALL.xlsx") %>%
  rename(value = `Final Value`, word = Term, code = Code) %>%
  select(-value) %>%
#Remove uninformative ngrams
  filter(code != 0)

#Load in sentences dataset
sent_proc <- b
#Add sentence id numbers
sent_id <- sent_proc %>%
  mutate(sentence_id = row_number()) 

#three has all of the trigrams from a sentence that matched one of the trigrams in the dictionary
three <- sent_id %>%
  unnest_tokens(word, text, token = "ngrams", n = 3) %>%
  inner_join(dict_all)

#Build sentaur3, which has the same columns as sent_id but just one row of NAs
sentaur3 <- sent_id
sentaur3[nrow(sentaur3)+1,] <- NA
sentaur3 <- sentaur3 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id)){
  #Filter matched trigrams to only those for the given sentence id
  a <- three %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>% 
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any trigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the trigram matches
    for(j in 1:nrow(a)){
    #Print trigram match
    print(a[j,]$word)
    #replace the trigram match from the given text with the word TRIGRAM
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "trigram"))
    }
    #Add to the running dataframe
    sentaur3 <- rbind(sentaur3, b)
  }
}

#Bind the sentences where matched trigrams replaced with TRIGRAM to the original sentence set
sent_id3 <- rbind(sentaur3, sent_id) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the TRIGRAM replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#two has all of the bigrams from a sentence that matched one of the bigrams in the dictionary
two <- sent_id3 %>%
  unnest_tokens(word, text, token = "ngrams", n = 2) %>%
  inner_join(dict_all) 

#Build sentaur2, which has the same columns as sent_id3 (and sent_id) but just one row of NAs
sentaur2 <- sent_id3
sentaur2[nrow(sentaur2)+1,] <- NA
sentaur2 <- sentaur2 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id3)){
  #Filter matched bigrams to only those for the given sentence id
  a <- two %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id3 %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>%
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any bigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the bigram matches
    for(j in 1:nrow(a)){
    #Print bigram match
    print(a[j,]$word)
    #remove the bigram match from the given text with the word BIGRAM
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "bigram"))
    }
    #Add to the running dataframe
    sentaur2 <- rbind(sentaur2, b)
  }
}

#Bind the sentences where matched bigrams replaced with BIGRAM to the set of distinct sentences that have TRIGRAM replacements where relevant
sent_id2 <- rbind(sentaur2, sent_id3) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the TRIGRAM replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#one has all of the unigrams from a sentence that matched one of the unigrams in the dictionary
one <- sent_id2 %>%
  unnest_tokens(word, text) %>%
  inner_join(dict_all)

#Sentence and Member Scoring -  sentences with no informative terms (NA) represented as arithmetic 0
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them sentence scores
  right_join(sent_id) %>%
  #Replace NA scores with 0
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_full0_ALLyears.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member, year) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -sentence_id, -year.y) %>%
  mutate(memberyear = paste(member, year.x, sep = " ")) %>%
  #Only keep one of the duplicated rows for each member
  distinct(memberyear, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_full0_ALLyears.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

#Sentence and Member Scoring -  sentences with no informative terms (NA) removed
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them scores where relevant (i.e. not NA)
  right_join(sent_id) %>%
  #Remove all scores of NA
  filter(!is.na(sent_score))

write_rds(sent_gram_scores, "sentences_fullNA_ALLyears.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member, year) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -sentence_id, -year.y) %>%
  mutate(memberyear = paste(member, year.x, sep = " ")) %>%
  #Only keep one of the duplicated rows for each member
  distinct(memberyear, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_fullNA_ALLyears.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))
```



```{r qualitative sentence finder}
#"I voted for tonight's legislation because it increases funding for border security, which will help prevent the flow of illegal immigration, human trafficking, and deadly drugs, including opioids and fentanyl, at our southern border"  https://votesmart.org//public-statement/1330679/house-passes-funding-for-border-security/?search=opioid#.Xrc6kxNKh0s
# "With these huge savings, we are also investing in new research for  new treatments and cures and fighting the opioid epidemic, as the  gentleman from New York (Mr Rose) pointed out, and in the community  health centers that deliver quality healthcare to so many Americans" https://votesmart.org//public-statement/1389259/lower-drug-costs-now-act-of-2019/?search=opioid
# "By deterring drug traffickers and those who produce illicit drugs, we are taking another step in a multi-faceted approach in the fight against fentanyl--and I'll keep working with my colleagues to prevent overdoses, increase access to wraparound treatment, and pave more pathways to long-term recovery" https://votesmart.org//public-statement/1376072/cassidy-hassan-kustoff-and-spanberger-introduce-bill-to-crack-down-on-illegal-pill-presses/?search=opioid#.XrdGFxNKh0s

sf <- read_rds("sentences_fullNA.rds")
s1 <- sf %>% filter(sent_score < .5 & sent_score > -.5) #%>% filter(str_detect(text, "treatment") & str_detect(text, "illicit"))
s1$text[143]

s1$text[2832]

s1 %>% filter(sentence_id == 647)

sf %>% filter(str_detect(text,"By deterring drug traffickers"))

```
