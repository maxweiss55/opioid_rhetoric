---
title: "PressReleases_Reprod"
author: "Max Weiss"
date: "5/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rvest)
library(ProPublicaR)
library(Rcrawler)
library(magrittr)
library(tidyr)
library(tokenizers)
library(htmlTable)
library(XML)
library(readxl)
library(tidytext)
library(htmltidy)
library(jsonlite)
library(lubridate)
library(textmineR)
library(tm)
library(fuzzyjoin)
library(reshape2)
```

```{r Member Frequency Analysis}

#Load in dataset with the date, title, and member for all 809,633 Public Statements scraped from VoteSmart on February 26, 2020
all <- read_rds('allstatements.rds') %>%
  #Remove empty rows
  filter(!is.na(date) & !is.na(title) & !is.na(member)) %>%
  #Remove all observations after February 17, 2020 (Opioid Public Statement dataset was scraped from VoteSmart on February 18, 2020)
  mutate(date = as.character(date)) %>%
  filter(date != "Feb. 18, 2020" &
         date != "Feb. 19, 2020" &
         date != "Feb. 20, 2020" &
         date != "Feb. 21, 2020" & 
         date != "Feb. 22, 2020" &
         date != "Feb. 23, 2020" &
         date != "Feb. 24, 2020" &           
         date != "Feb. 25, 2020" &           
         date != "Feb. 26, 2020") 
#809,170 Public Statements remain at this point
      
#Count the number of Public Statements from each member
all_memcount <- all %>%
  #Remove observations that are full repeats; 21,010 (2.60%) repeat observations removed in this step
  distinct() %>%
  #788,160 Public Statememnts remain at this point
  #Tally the number of observations grouped by member
  count(member) %>%
  #Arrange from highest to lowest member count
  arrange(desc(n)) %>%
  #Rename column displaying member counts as `total`
  rename(total = n)

#Load in dataset with the date, title, member, path, and text for the 13,500 Opioid Public Statements scraped from VoteSmart on February 18, 2020
opioid <- read_rds('fullscrape.rds') %>%
  #Remove empty rows
  filter(!is.na(date) & !is.na(title) & !is.na(member) & !is.na(path) & !is.na(text)) %>%
  #Remove all observations after February 17, 2020 (Opioid Public Statement dataset was scraped from VoteSmart on February 18, 2020)
  mutate(date = as.character(date)) %>%
  filter(date != "Feb. 18, 2020")
#13,497 Opioid Public Statements remain at this point

#Count the number of Opioid Public Statements from each member
opioid_memcount <- opioid %>%
  #Remove observations that are full repeats; 422 (3.13%) repeat observations removed in this step
  distinct() %>%
  #13,075 Opioid Public Atatememnts remain at this point
  #Tally the number of observations grouped by member
  count(member) %>%
  #Arrange from highest to lowest member count
  arrange(desc(n)) %>%
  #Rename column displaying member counts as `opioid`
  rename(opioid = n)

#Find the proportion of each member's total Public Statements that are related to opioids
#Join the two informative dataframes (defaults by member)
memcounts <- left_join(all_memcount, opioid_memcount) %>%
  #If none of a member's Public Statements were about opioids, set the `opioid` variable equal to 0 (instead of NA)
  mutate(opioid = ifelse(is.na(opioid), 0, opioid)) %>%
  #Divide for each member: number of Public Statements about opioids divided by total number of Public Statements
  mutate(prop = opioid / total) %>%
  #Arrange from highest to lowest relative frequency
  arrange(desc(prop))

#Load in the key that relates congress member identifiers and information `full name` to their processed (special characters removed, all lowercase) name from VoteSmart `member`
#This key was constructed manually for all 536 voting members of the 116th Congress
legkey <- read_csv("legkey.csv")

#Join the member relative frequency dataset to member key to add standardized identifiers
memcounts_indexed <- memcounts %>%
  #Process `member` to remove special characters and lowercase, match key `member` column
  #Remove "Sen." and "Rep." prefixes from `member`
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  #`member` to lowercase
  mutate(member = str_to_lower(member)) %>%
  #Remove accents and tilde in `member`
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  #Parse double spaces to single spaces in `member`
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  #Parse "’" symbol string to "'" symbol string in `member`
  mutate(member = str_replace_all(member, "’", "'")) %>%
  #Join to member key by `member` 
  left_join(legkey, by = "member")

#Save dataframe to .rds file
write_rds(memcounts_indexed, "freq_allvotesmart.rds")

```

```{r Member Frequency Analysis for 116th Congress}

#Load in dataset with the date, title, and member for all 809,633 Public Statements scraped from VoteSmart on February 26, 2020
all <- read_rds('allstatements.rds') %>%
  #Remove empty rows
  filter(!is.na(date) & !is.na(title) & !is.na(member)) %>%
  #Filter to include only Public Statements from the first year of the 116th Congress
  mutate(date = as.character(date)) %>%
  filter(str_detect(date, "2019") | 
         str_detect(date, "Jan. 3, 2020") | 
         str_detect(date, "Jan. 2, 2020") | 
         str_detect(date, "Jan. 1, 2020")) %>%
  filter(!str_detect(date, "Jan. 2, 2019") & 
         !str_detect(date, "Jan. 1, 2019")) 
#111,014 Public Statements remain at this point

#Count the number of Public Statements from each member
all_memcount <- all %>%
  #Remove observations that are full repeats; 4,368 (3.93%) repeat observations removed in this step
  distinct() %>%
  #106,646 Public Statememnts remain at this point
  #Tally the number of observations grouped by member
  count(member) %>%
  #Arrange from highest to lowest member count
  arrange(desc(n)) %>%
  #Rename column displaying member counts as `total`
  rename(total = n)

#Load in dataset with the date, title, member, path, and text for the 13,500 Opioid Public Statements scraped from VoteSmart on February 18, 2020
opioid <- read_rds('fullscrape.rds') %>%
  #Remove empty rows
  filter(!is.na(date) & !is.na(title) & !is.na(member) & !is.na(path) & !is.na(text)) %>%
  #Filter to include only Public Statements from the first year of the 116th Congress
  mutate(date = as.character(date)) %>%
  filter(str_detect(date, "2019") | 
         str_detect(date, "Jan. 3, 2020") | 
         str_detect(date, "Jan. 2, 2020") | 
         str_detect(date, "Jan. 1, 2020")) %>%
  filter(!str_detect(date, "Jan. 2, 2019") & 
         !str_detect(date, "Jan. 1, 2019"))
#2,383 Opioid Public Statements remain at this point

#Count the number of Opioid Public Statements from each member
opioid_memcount <- opioid %>%
  #Remove observations that are full repeats; 136 (5.71%) repeat observations removed in this step
  distinct() %>%
  #2,247 Opioid Public Atatememnts remain at this point
  #Tally the number of observations grouped by member
  count(member) %>%
  #Arrange from highest to lowest member count
  arrange(desc(n)) %>%
  #Rename column displaying member counts as `opioid`
  rename(opioid = n)

#Find the proportion of each member's total Public Statements that are related to opioids
#Join the two informative dataframes (defaults by member)
memcounts <- left_join(all_memcount, opioid_memcount) %>%
  #If none of a member's Public Statements were about opioids, set the `opioid` variable equal to 0 (instead of NA)
  mutate(opioid = ifelse(is.na(opioid), 0, opioid)) %>%
  #Divide for each member: number of Public Statements about opioids divided by total number of Public Statements
  mutate(prop = opioid / total) %>%
  #Arrange from highest to lowest relative frequency
  arrange(desc(prop))

#Load in the key that relates congress member identifiers and information `full name` to their processed (special characters removed, all lowercase) name from VoteSmart `member`
#This key was constructed manually for all 536 voting members of the 116th Congress
legkey <- read_csv("legkey.csv")

#Join the member relative frequency dataset to member key to add standardized identifiers
memcounts_indexed <- memcounts %>%
  #Process `member` to remove special characters and lowercase, match key `member` column
  #Remove "Sen." and "Rep." prefixes from `member`
  mutate(member = str_remove_all(member, "Sen\\. ")) %>%
  mutate(member = str_remove_all(member, "Rep\\. ")) %>%
  #`member` to lowercase
  mutate(member = str_to_lower(member)) %>%
  #Remove accents and tilde in `member`
  mutate(member = str_replace_all(member, 'á', 'a')) %>%
  mutate(member = str_replace_all(member, 'é', 'e')) %>%
  mutate(member = str_replace_all(member, 'ó', 'o')) %>%
  mutate(member = str_replace_all(member, 'í', 'i')) %>%
  mutate(member = str_replace_all(member, 'ú', 'u')) %>%
  mutate(member = str_replace_all(member, 'ñ', 'n')) %>%
  #Parse double spaces to single spaces in `member`
  mutate(member = str_replace_all(member, '  ', ' '))  %>%
  #Parse "’" symbol string to "'" symbol string in `member`
  mutate(member = str_replace_all(member, "’", "'")) %>%
  #Join to member key by `member` 
  left_join(legkey, by = "member")
  #Note: Sen. Kelly Loeffler was not sworn into office until January 6, 2020, so no statements from Sen. Loeffler were included in this timeframe

#Save dataframe to .rds file
write_rds(memcounts_indexed, "freq_116th.rds")

```

```{r Build Sentences Dataset 116th}

#Load in dataset with the date, title, member, path, and text for the 13,500 Opioid Public Statements scraped from VoteSmart on February 18, 2020
opioid <- read_rds('fullscrape.rds') %>%
  #Remove empty rows
  filter(!is.na(date) & !is.na(title) & !is.na(member) & !is.na(path) & !is.na(text)) %>%
  #Filter to include only Public Statements from the first year of the 116th Congress
  mutate(date = as.character(date)) %>%
  filter(str_detect(date, "2019") | 
         str_detect(date, "Jan. 3, 2020") | 
         str_detect(date, "Jan. 2, 2020") | 
         str_detect(date, "Jan. 1, 2020")) %>%
  filter(!str_detect(date, "Jan. 2, 2019") & 
         !str_detect(date, "Jan. 1, 2019")) %>%
  #Remove phrase that ended the scraped text of most public statements
  mutate(text = str_remove_all(text, " All content © 1992 - 2020 Vote Smart unless otherwise attributed - Privacy Policy - Legislative demographic data provided by Aristotle International, Inc. Mobile Version \\#\\{text\\} You are about to be redirected to a secure checkout page. Please note: The total order amount will read \\$0.01. This is a card processor fee. Please know that a recurring donation of the amount and frequency that you selected will be processed and initiated tomorrow. You may see a one-time charge of \\$0.01 on your statement. Continue to secure page »")) %>%
  #Remove observations that are full repeats
  distinct()
#2,247 Opioid Public Statements remain at this point

#Split the text of each Opioid Public Statements by sentence
opioid_sentences <- opioid %>%
  #Remove periods that directly follow a capital letter (likely to be an acronym)
  mutate(text = str_replace_all(text, "(?<=[:upper:])\\.", "")) %>%
  #Remove periods that are both directly preceded by and directly follow a lowercase letter (likely to be an acronym)
  mutate(text = str_replace_all(text, "(?<=[:alnum:])\\.(?=[:alnum:])", "")) %>%
  #Remove the periods from common abbreviations derived through sampling the dataset
  mutate(text = str_replace_all(text, "Sen\\.", "Sen")) %>%
  mutate(text = str_replace_all(text, "Rep\\.", "Rep")) %>%
  mutate(text = str_replace_all(text, "Sens\\.", "Sens")) %>%
  mutate(text = str_replace_all(text, "Reps\\.", "Reps")) %>%
  mutate(text = str_replace_all(text, "Mrs\\.", "Mrs")) %>%
  mutate(text = str_replace_all(text, "Mr\\.", "Mr")) %>%
  mutate(text = str_replace_all(text, "Ms\\.", "Ms")) %>%
  mutate(text = str_replace_all(text, "Dr\\.", "Dr")) %>%
  mutate(text = str_replace_all(text, "Pres\\.", "Pres")) %>%
  mutate(text = str_replace_all(text, "St\\.", "St")) %>%
  #Separate Public Statements by end punctuation (periods, question marks, exclammation point) and create a new observation for each separated sentence
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  #121,670 split statements derived
  #Filter to only include split statements that contain language directly associated with opioids
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|narcan|synthetics|oxycodone|hydrocodone|morphine|codeine|methadone|meperidine|buprenorphine|hydromorphone|benzo", ignore_case = TRUE))) %>%
  #Add a sentence identifier based on row number
  mutate(sentence_id = row_number())
  #10,404 split statements about opioids remain

#The follow seeks to correct for split statements that are especially long, primarily due to the practice of including long lists in public statements that are not separated by common end punctuation. These heuristics were discovered through sampling the longest split statements.

#Build histogram of split statement lengths
length <- opioid_sentences %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)
hist(length$count, breaks = 1000)

opioid_sentences2 <- opioid_sentences

#Replace all dollar signs directly preceded by a lowercase letter with a period (prepare for splitting) followed by a dollar sign. This is an uncommon string pattern that was a feature of lists detailing where money is being allocated, where entries were split by formatting rather than a traditional end punctuation, and should be treated as separate statements. This was a feature of 30 split statements.
opioid_sentences2 <- opioid_sentences2 %>%
  mutate(text = str_replace_all(text, "(?<=[:lower:])\\$", ".$"))

#Replace all semicolons directly followed by a non-space character with a semicolon followed by a period (prepare for splitting). This is an uncommon string pattern that was a feature of lists, where entries were not split by traditional end punctuation, and should be treated as separate statements. This was a feature of 111 split statements.
opioid_sentences2 <- opioid_sentences2 %>%
  mutate(text = str_replace_all(text, "\\;(?=[:graph:])", ";."))

#Replace the bullet point symbol found in many long lists with a period (prepare for splitting). This is an uncommon string pattern that was a feature of lists and should be treated as separate statements. This was a feature of 8 split statements.
opioid_sentences2 <- opioid_sentences2 %>%
  mutate(text = str_replace_all(text, "·", "."))

#Separate Public Statements by end punctuation (periods, question marks, exclammation point) and create a new observation for each separated sentence
#Filter to only include sentences that contain language directly associated with opioids
opioid_sentences2 <- opioid_sentences2 %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|narcan|synthetics|oxycodone|hydrocodone|morphine|codeine|methadone|meperidine|buprenorphine|hydromorphone|benzo", ignore_case = TRUE))) 

#One more round of sampling was completed for the longest text strings, and another heuristic was discovered: many of the longest text strings were long because they were from scraped public statements that contained two statements separated by formatting alone (not by any punctuation). These were, therefore, parsed as the last word of one statement concatenated to the first word of another statement, so they often contained a lowercase letter followed by an uppercase letter. This string pattern is uncommon, but it is more common than those in the previous heuristics (for example, the pattern would appear in "McConnell"). To separate long statements that should be represented as separate sentences, while mitigating potential for error, the pattern was only remedied in the longest 1% of text strings.
#Place a period (prepare for splitting) in the middle of the pattern of a lowercase letter followed directly by an uppercase letter. This was a feature of 88 observations that were in the top 1% of observations by length.
opioid_sentences2 <- opioid_sentences2 %>%
  #Count the length of each text string
  mutate(count = str_count(text)) %>%
  #If, the string length is greater than 793 characters, add a period in between the pattern of interest. 1% of text strings were greater than 793 characters.
  mutate(text = ifelse(count > 793, str_replace_all(text, "(?<=[:lower:])(?=[:upper:])", "."), text)) %>%
  #Remove string count variable (unnecessary for future analysis)
  select(-count)

#Separate Public Statements by end punctuation (periods, question marks, exclammation point) and create a new observation for each separated sentence
#Filter to only include sentences that contain language directly associated with opioids
opioid_sentences2 <- opioid_sentences2 %>%
  separate_rows(text, sep = "[.?!]", convert = FALSE) %>%
  filter(str_detect(text, regex("heroin|opioid|opiate|fentanyl|naloxone|narcan|synthetics|oxycodone|hydrocodone|morphine|codeine|methadone|meperidine|buprenorphine|hydromorphone|benzo", ignore_case = TRUE))) %>%
  mutate(sentence_id = row_number())
#10,638 split statements (hereafter referred as sentences) remain

#Build histogram of sentence lengths
length2 <- opioid_sentences2 %>%
  mutate(count = str_count(text)) %>%
  arrange(desc(count)) %>%
  select(text, count)
hist(length2$count, breaks = 1000)

#Save dataframe to .rds file
write_rds(opioid_sentences2, "sentences_116th.rds")

```


```{r Scoring Function}

#Build function (score_martix) that returns scoring matrix object (sentence_matrix) for any dataset with columns `text` (text of each document). The resulting dataframe (sentence_matrix) provides all the necessary components for calculating the scores of each sentence.
#Note: This function requires the dictionary file "Dictionary_ALL.xlsx" in the working directory. This dataframe includes two columns, one column with all unigrams, bigrams, and trigrams and a second column with the coded value (-1, 0, 1) for each associated ngram

score_matrix <- function (documents_dataframe) {

#Load full dictionary
dict_all <- read_xlsx("Dictionary_ALL.xlsx") %>%
  rename(word = Term, code = Code) %>%
#Remove uninformative ngrams
  filter(code != 0)

#Load in sentences dataset
sent_proc <- documents_dataframe
#Add sentence id numbers
sent_id <- sent_proc %>%
  mutate(sentence_id = row_number())

#"three" has all of the trigrams from a sentence that matched one of the trigrams in the dictionary
three <- sent_id %>%
  unnest_tokens(word, text, token = "ngrams", n = 3) %>%
  inner_join(dict_all)

#Build sentaur3, which has the same columns as sent_id but just one row of NAs
sentaur3 <- sent_id
sentaur3[nrow(sentaur3)+1,] <- NA
sentaur3 <- sentaur3 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id)){
  #Filter matched trigrams to only those for the given sentence id
  a <- three %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>% 
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any trigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the trigram matches
    for(j in 1:nrow(a)){
    #Print trigram match
    print(a[j,]$word)
    #replace the trigram match from the given text with the word "trigram"
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "trigram"))
    }
    #Add to the running dataframe
    sentaur3 <- rbind(sentaur3, b)
  }
}

#Bind the sentences where matched trigrams replaced with "trigram" to the original sentence set
sent_id3 <- rbind(sentaur3, sent_id) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the "trigram" replaced sentences if duplicate (because these come first)
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#"two" has all of the bigrams from a sentence that matched one of the bigrams in the dictionary
two <- sent_id3 %>%
  unnest_tokens(word, text, token = "ngrams", n = 2) %>%
  inner_join(dict_all) 

#Build sentaur2, which has the same columns as sent_id3 (and sent_id) but just one row of NAs
sentaur2 <- sent_id3
sentaur2[nrow(sentaur2)+1,] <- NA
sentaur2 <- sentaur2 %>% filter(is.na(sentence_id))
#Loop through each sentence id
for (i in 1:nrow(sent_id3)){
  #Filter matched bigrams to only those for the given sentence id
  a <- two %>%
    filter(sentence_id == i)
  #Filter sentences to only that of the given sentence id
  b <- sent_id3 %>%
    filter(sentence_id == i) %>%
    #remove everything that is not a space or letter
    mutate(text = str_replace_all(text, '[^[:alpha:][:space:]]', " ")) %>%
    mutate(text = str_to_lower(text)) #text to lowercase
  #IF there are any bigram matches, then...
  if (nrow(a) > 0){
    #print the sent_id number
    print(i)
    #loop through each of the bigram matches
    for(j in 1:nrow(a)){
    #Print bigram match
    print(a[j,]$word)
    #remove the bigram match from the given text with the word bigram
    b <- b %>%
      mutate(text = str_replace(text, a[j,]$word, "bigram"))
    }
    #Add to the running dataframe
    sentaur2 <- rbind(sentaur2, b)
  }
}

#Bind the sentences where matched bigrams replaced with bigram to the set of distinct sentences that have TRIGRAM replacements where relevant
sent_id2 <- rbind(sentaur2, sent_id3) %>%
  #Only keep the first of a duplicate sentence_id, this keeps only the bigram replaced sentences if duplicate because these come first
  distinct(sentence_id, .keep_all = TRUE) %>%
  #Remove the blank row used to start table
  filter(!is.na(sentence_id))

#"one" has all of the unigrams from a sentence that matched one of the unigrams in the dictionary
one <- sent_id2 %>%
  unnest_tokens(word, text) %>%
  inner_join(dict_all)

sent_matrix <- rbind(three, two, one)
return(sent_matrix)
}

df <- read_rds("sentences_116th.rds")
sentence_matrix <- score_matrix(df)

```







#Sentence and Member Scoring -  sentences with no informative terms (NA) represented as arithmetic 0
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them sentence scores
  right_join(sent_id) %>%
  #Replace NA scores with 0
  mutate(sent_score = ifelse(is.na(sent_score), 0, sent_score))

write_rds(sent_gram_scores, "sentences_full0.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  #Only keep one of the duplicated rows for each member
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_full0.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))

#Sentence and Member Scoring -  sentences with no informative terms (NA) removed
#Score sentences
sent_gram_scores <- rbind(three, two, one) %>%
  #Group by sentence
  group_by(sentence_id) %>%
  #Take the mean of the code from each ngram in the sentence
  summarise(sent_score = mean(code)) %>%
  #Retrieve all original sentences and join to them scores where relevant (i.e. not NA)
  right_join(sent_id) %>%
  #Remove all scores of NA
  filter(!is.na(sent_score))

write_rds(sent_gram_scores, "sentences_fullNA.rds")

#Score members
gram_mem_scores <- sent_gram_scores %>%
  #Group by member
  group_by(member) %>%
  #Take the mean of each member
  summarise(mem_score = mean(sent_score)) %>%
  #Retrieve all original sentences and join to them MEMBER scores (this is only to retrieve the member information, we end up with many redundant rows which will be deleted)
  right_join(sent_id, by = "member") %>%
  #Remove members not represented in the sentence scoring
  filter(!is.na(mem_score)) %>%
  #Remove unimportant columns
  select(-date, -title, -path, -text, -year, -day, -month, -sentence_id) %>%
  #Only keep one of the duplicated rows for each member
  distinct(member, .keep_all = TRUE)

write_rds(gram_mem_scores, "members_fullNA.rds")

hist(sent_gram_scores$sent_score)
hist(gram_mem_scores$mem_score)
gram_mem_scores %>% group_by(party) %>% summarise(mean(mem_score))
z <- read_rds("sentences_fullNA.rds")

y <- z %>% filter(sentence_id == 9123)
rbind(three, two, one) %>% filter(sentence_id == 9123)
y$text

sent_id3 %>% filter(sentence_id == 8993)

```